{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized = pd.read_csv('../csv_files/mlb_normalized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_year = []\n",
    "count_all_year = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_accuracy_year(year):\n",
    "    normalized_train_year = df_normalized[df_normalized['season'] == year]\n",
    "    normalized_train_year_x = normalized_train_year[normalized_train_year.columns[1:13]]\n",
    "    normalized_train_year_y_score_1 = normalized_train_year[normalized_train_year.columns[14:15]]\n",
    "    normalized_train_year_y_score_2 = normalized_train_year[normalized_train_year.columns[15:16]]\n",
    "\n",
    "    length_min = 0\n",
    "    length_max = len(normalized_train_year_x)\n",
    "    count_year.append(length_max)\n",
    "\n",
    "    predicted_score_1 = []\n",
    "    predicted_score_2 = []\n",
    "    regr_score_1 = linear_model.LinearRegression()\n",
    "    regr_score_1.fit(normalized_train_year_x, normalized_train_year_y_score_1)\n",
    "    for i in range(length_min,length_max):\n",
    "        predicted_score_1.append(regr_score_1.predict([normalized_train_year_x.iloc[i]]))\n",
    "\n",
    "    regr_score_2 = linear_model.LinearRegression()\n",
    "    regr_score_2.fit(normalized_train_year_x, normalized_train_year_y_score_2)\n",
    "    for i in range(length_min,length_max):\n",
    "        predicted_score_2.append(regr_score_2.predict([normalized_train_year_x.iloc[i]]))\n",
    "    results = normalized_train_year[normalized_train_year.columns[16:17]]\n",
    "    results = results[length_min:length_max]\n",
    "\n",
    "    generated_results = []\n",
    "\n",
    "    for i in range(length_max - length_min):\n",
    "        if predicted_score_1[i] < predicted_score_2[i]:\n",
    "            generated_results.append(1)\n",
    "        else:\n",
    "            generated_results.append(0)\n",
    "    Accuracy = metrics.accuracy_score(generated_results, results)\n",
    "    return Accuracy * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_accuracy_all_years():\n",
    "    normalized_train_year = df_normalized\n",
    "    normalized_train_year_x = normalized_train_year[normalized_train_year.columns[1:13]]\n",
    "    normalized_train_year_y_score_1 = normalized_train_year[normalized_train_year.columns[14:15]]\n",
    "    normalized_train_year_y_score_2 = normalized_train_year[normalized_train_year.columns[15:16]]\n",
    "\n",
    "    length_min = 0\n",
    "    length_max = len(normalized_train_year_x)\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    regr_score_1 = linear_model.LinearRegression()\n",
    "    regr_score_1.fit(normalized_train_year_x, normalized_train_year_y_score_1)\n",
    "\n",
    "    regr_score_2 = linear_model.LinearRegression()\n",
    "    regr_score_2.fit(normalized_train_year_x, normalized_train_year_y_score_2)\n",
    "\n",
    "    for y in range(1913, 2021):\n",
    "\n",
    "        temp_test_year = normalized_train_year_x[normalized_train_year_x['season'] == y]\n",
    "        temp_results_year = normalized_train_year[normalized_train_year['season'] == y]\n",
    "        \n",
    "        length_min = 0\n",
    "        length_max = len(temp_test_year)\n",
    "        count_all_year.append(length_max)\n",
    "\n",
    "        \n",
    "        \n",
    "        predicted_score_1 = []\n",
    "        predicted_score_2 = []\n",
    "\n",
    "        for i in range(length_min,length_max):\n",
    "            predicted_score_1.append(regr_score_1.predict([temp_test_year.iloc[i]]))\n",
    "\n",
    "        for i in range(length_min,length_max):\n",
    "            predicted_score_2.append(regr_score_2.predict([temp_test_year.iloc[i]]))\n",
    "\n",
    "\n",
    "        results = temp_results_year[temp_results_year.columns[16:17]]\n",
    "        results = results[length_min:length_max]\n",
    "\n",
    "        generated_results = []\n",
    "\n",
    "        for i in range(length_max - length_min):\n",
    "            if predicted_score_1[i] < predicted_score_2[i]:\n",
    "                generated_results.append(1)\n",
    "            else:\n",
    "                generated_results.append(0)\n",
    "\n",
    "        confusion_matrix = metrics.confusion_matrix(generated_results, results)\n",
    "\n",
    "        cm_display = metrics.ConfusionMatrixDisplay(\n",
    "            confusion_matrix=confusion_matrix, display_labels=[False, True])\n",
    "\n",
    "        cm_display.plot()\n",
    "        plt.show()\n",
    "        Accuracy = metrics.accuracy_score(generated_results, results)\n",
    "        data[y] = Accuracy * 100\n",
    "    \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "for i in range(1913, 2021):\n",
    "    data[i] = get_model_accuracy_year(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = get_model_accuracy_all_years()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph shows the models accuracy when trained and tested with data from individual seasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph shows the models accuracy when trained with all of the data and tested against each individual season."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a stark difference between the accuracy of the regression model when trained with individual seasons and the model trained with all of the data. Both models are tested with individal seasons. These results would make sense because a model trained with more relevant and specific data will return better results when tested with the same relevant data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the accuracy per season creats this sin wave when plotted by year and accuracy when the model is ran through that year\n",
    "I found this really interesting and wanted to explore what caused this shape.\n",
    "Seems that the there is an obvious change in direction in the exact center. The center of the given years is 1967.\n",
    "To further explore lets plot out the different values and compare values from before and after the year 1967.\n",
    "Its also interesting to point out that Q1 and Q3 are more empty than Q2 and Q4."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
